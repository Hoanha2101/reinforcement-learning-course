# Assignment 2 - Q-Learning and Expected Sarsa

## ğŸ“œ Giá»›i thiá»‡u

BÃ i táº­p nÃ y yÃªu cáº§u báº¡n triá»ƒn khai vÃ  so sÃ¡nh hai thuáº­t toÃ¡n **Q-Learning** vÃ  **Expected Sarsa** trÃªn bÃ i toÃ¡n **Cliff World**.

### ğŸ¯ Má»¥c tiÃªu bÃ i táº­p
1. **Triá»ƒn khai Q-Learning** vá»›i chÃ­nh sÃ¡ch **Îµ-greedy**.
2. **Triá»ƒn khai Expected Sarsa** vá»›i chÃ­nh sÃ¡ch **Îµ-greedy**.
3. **PhÃ¢n tÃ­ch hÃ nh vi cá»§a hai thuáº­t toÃ¡n** trÃªn mÃ´i trÆ°á»ng **Cliff World** (mÃ´ táº£ á»Ÿ trang 132 cá»§a sÃ¡ch giÃ¡o khoa).

ChÃºng tÃ´i sáº½ cung cáº¥p mÃ´i trÆ°á»ng vÃ  háº¡ táº§ng Ä‘á»ƒ cháº¡y thÃ­ nghiá»‡m vÃ  trá»±c quan hÃ³a káº¿t quáº£. **BÃ i táº­p sáº½ Ä‘Æ°á»£c cháº¥m Ä‘iá»ƒm tá»± Ä‘á»™ng**, báº±ng cÃ¡ch so sÃ¡nh hÃ nh vi cá»§a agent vá»›i cÃ¡c triá»ƒn khai chuáº©n cá»§a **Expected Sarsa** vÃ  **Q-Learning**.

Äá»ƒ trÃ¡nh káº¿t quáº£ khÃ¡c nhau do ngáº«u nhiÃªn, **random seed sáº½ Ä‘Æ°á»£c Ä‘áº·t cá»‘ Ä‘á»‹nh**.

---

## ğŸ— MÃ´ táº£ bÃ i toÃ¡n **Cliff World**

- **MÃ´i trÆ°á»ng Cliff World** lÃ  má»™t lÆ°á»›i, nÆ¡i mÃ  agent pháº£i di chuyá»ƒn tá»« **vá»‹ trÃ­ xuáº¥t phÃ¡t (Start)** Ä‘áº¿n **Ä‘Ã­ch (Goal)**.
- **HÃ nh Ä‘á»™ng:** Agent cÃ³ thá»ƒ di chuyá»ƒn **LÃªn, Xuá»‘ng, TrÃ¡i, Pháº£i**.
- **Tráº¡ng thÃ¡i Ä‘áº·c biá»‡t:**
  - Náº¿u agent **rÆ¡i xuá»‘ng vá»±c (Cliff)**, nÃ³ nháº­n **pháº§n thÆ°á»Ÿng -100** vÃ  quay láº¡i vá»‹ trÃ­ xuáº¥t phÃ¡t.
  - Náº¿u agent **Ä‘áº¿n Ä‘Ã­ch (Goal)**, nÃ³ nháº­n **pháº§n thÆ°á»Ÿng 0** vÃ  káº¿t thÃºc táº­p.
  - CÃ¡c bÆ°á»›c Ä‘i thÃ´ng thÆ°á»ng cÃ³ **pháº§n thÆ°á»Ÿng -1**.

Má»¥c tiÃªu cá»§a agent lÃ  **há»c cÃ¡ch Ä‘áº¿n Ä‘Ã­ch nhanh nháº¥t cÃ³ thá»ƒ**, trÃ¡nh rÆ¡i xuá»‘ng vá»±c.

---

## ğŸ”¢ CÃ¡c thuáº­t toÃ¡n sá»­ dá»¥ng

### âœ… **Q-Learning**
- **LÃ  thuáº­t toÃ¡n Off-Policy**, cáº­p nháº­t giÃ¡ trá»‹ tráº¡ng thÃ¡i-hÃ nh Ä‘á»™ng theo cÃ´ng thá»©c:

  $$ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] $$

- Agent cáº­p nháº­t giÃ¡ trá»‹ **dá»±a trÃªn hÃ nh Ä‘á»™ng tá»‘t nháº¥t cÃ³ thá»ƒ**, thay vÃ¬ hÃ nh Ä‘á»™ng thá»±c táº¿ Ä‘Ã£ chá»n.

### âœ… **Expected Sarsa**
- **LÃ  thuáº­t toÃ¡n On-Policy**, cáº­p nháº­t giÃ¡ trá»‹ theo **trung bÃ¬nh cÃ³ trá»ng sá»‘** cá»§a táº¥t cáº£ cÃ¡c hÃ nh Ä‘á»™ng kháº£ thi:

  $$ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \sum_{a'} \pi(a'|s') Q(s', a') - Q(s, a)] $$

- Agent **sá»­ dá»¥ng chÃ­nh sÃ¡ch Ï€ hiá»‡n táº¡i** Ä‘á»ƒ cáº­p nháº­t giÃ¡ trá»‹ tráº¡ng thÃ¡i-hÃ nh Ä‘á»™ng.

---

## ğŸ“¦ ThÆ° viá»‡n sá»­ dá»¥ng
BÃ i táº­p nÃ y sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n sau:
1. **numpy**: ThÆ° viá»‡n toÃ¡n há»c cho Python.
2. **matplotlib**: Há»— trá»£ váº½ Ä‘á»“ thá»‹ káº¿t quáº£.
3. **RL-Glue**: Framework Ä‘á»ƒ cháº¡y cÃ¡c thÃ­ nghiá»‡m há»c tÄƒng cÆ°á»ng.

**âš ï¸ LÆ°u Ã½ quan trá»ng:**
- **KhÃ´ng nháº­p thÃªm thÆ° viá»‡n khÃ¡c**, vÃ¬ Ä‘iá»u nÃ y cÃ³ thá»ƒ lÃ m há»ng bá»™ cháº¥m Ä‘iá»ƒm tá»± Ä‘á»™ng.
- **LÃ m theo Ä‘Ãºng trÃ¬nh tá»± cÃ¡c Ã´ mÃ£** Ä‘á»ƒ Ä‘áº£m báº£o thuáº­t toÃ¡n hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c.
- **Sá»­ dá»¥ng Ä‘Ãºng hÃ m Ä‘á»ƒ táº¡o sá»‘ ngáº«u nhiÃªn**, vÃ  **khÃ´ng thay Ä‘á»•i sá»‘ láº§n gá»i hÃ m ngáº«u nhiÃªn**, vÃ¬ Ä‘iá»u nÃ y cÃ³ thá»ƒ lÃ m sai lá»‡ch káº¿t quáº£.

---

## ğŸ“Œ Ghi chÃº
- **Q-Learning** thÆ°á»ng há»™i tá»¥ nhanh hÆ¡n nhÆ°ng khÃ´ng á»•n Ä‘á»‹nh báº±ng **Expected Sarsa**.
- **Expected Sarsa** thÆ°á»ng hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n trong cÃ¡c mÃ´i trÆ°á»ng cÃ³ nhiá»…u, vÃ¬ nÃ³ cáº­p nháº­t dá»±a trÃªn chÃ­nh sÃ¡ch hiá»‡n táº¡i.
- So sÃ¡nh hai thuáº­t toÃ¡n sáº½ giÃºp báº¡n hiá»ƒu rÃµ hÆ¡n vá» sá»± khÃ¡c biá»‡t giá»¯a **On-Policy** vÃ  **Off-Policy**.

---

## ğŸ“œ Báº£n quyá»n
BÃ i táº­p nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a trÃªn cÃ¡c tÃ i liá»‡u vá» **Reinforcement Learning** cá»§a **Richard Sutton**.