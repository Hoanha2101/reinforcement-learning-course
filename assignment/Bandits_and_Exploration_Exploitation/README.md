# Assignment 1: Bandits and Exploration/Exploitation

## 📜 Giới thiệu

Bài tập này sẽ giúp bạn:
- Tạo thuật toán bandit đầu tiên của mình.
- Hiểu rõ ảnh hưởng của **epsilon** lên sự cân bằng giữa **khám phá (exploration)** và **khai thác (exploitation)**.
- Làm quen với một số thuật toán học tăng cường sẽ được sử dụng trong chuyên ngành này.

Lớp học này sử dụng **RL-Glue** để triển khai hầu hết các thí nghiệm. Thư viện này được thiết kế bởi **Adam White, Brian Tanner và Rich Sutton** nhằm cung cấp một **framework vững chắc** để hiểu cách hoạt động của các thí nghiệm trong **Reinforcement Learning**.

Nếu bạn chưa từng sử dụng **Jupyter Notebook** trước đây, bạn có thể chỉ cần nhấn nút **Run**, hoặc bấm **Shift + Enter** để chạy từng ô mã.

---

## 🛠 Phần chuẩn bị

Trước tiên, ta cần nhập các thư viện cần thiết:

```python
# Import necessary libraries
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import time

from rlglue.rl_glue import RLGlue
import main_agent
import ten_arm_env
import test_env
```

### 🔍 Giải thích các thư viện
- **`numpy`**: Thư viện toán học phổ biến.
- **`matplotlib.pyplot`**: Hỗ trợ vẽ đồ thị.
- **`tqdm`**: Thanh tiến trình giúp theo dõi quá trình huấn luyện.
- **`time`**: Dùng để theo dõi thời gian thực thi.
- **`rlglue.rl_glue`**: Framework RL-Glue giúp quản lý thí nghiệm RL.
- **`main_agent`**: Agent sẽ được triển khai.
- **`ten_arm_env`**: **Môi trường 10-armed Testbed** – Một bài toán bandit phổ biến.
- **`test_env`**: Môi trường kiểm thử khác.

### ⚠️ Lưu ý quan trọng
- **Không nhập thêm thư viện khác** vì điều này có thể ảnh hưởng đến bộ chấm điểm tự động.
- **Không đặt random seed** vì điều này có thể làm hỏng bộ chấm điểm.
- **Không nhân đôi ô mã**, vì điều này có thể đặt notebook vào trạng thái xấu.

Trước khi nộp bài, hãy chạy:

```
Kernel -> Restart and Run All
```

để đảm bảo tất cả các ô mã đều chạy chính xác.

---

## 🎯 Mô tả bài toán **10-Armed Testbed**

Bài tập này sử dụng **10-armed Testbed**, được giới thiệu trong **Section 2.3 của sách giáo khoa**. Đây là một môi trường bandit gồm **10 cánh tay** (**actions**).

- Khi kéo một cánh tay, phần thưởng nhận được là **ngẫu nhiên**, được lấy từ phân phối Gaussian.
- Mỗi hành động có **giá trị kỳ vọng riêng**, được khởi tạo ngẫu nhiên từ phân phối chuẩn **N(0,1)**.
- Mục tiêu của bài toán là tìm cách **tối ưu hóa giá trị kỳ vọng bằng cách cân bằng giữa khai thác và khám phá**.


---

## 📌 Ghi chú
- Nếu bạn chưa quen với **10-armed Testbed**, hãy đọc lại phần **Section 2.3 của sách giáo khoa**.
- Thuật toán **Epsilon-Greedy** sẽ được sử dụng để kiểm tra sự ảnh hưởng của **epsilon** lên hiệu suất học tập.
- Bạn có thể thay đổi **giá trị epsilon** để quan sát sự thay đổi giữa **khám phá** và **khai thác**.

---

## 📜 Bản quyền
Bài tập này được phát triển dựa trên nội dung từ **RL-Glue** và các tài liệu về Reinforcement Learning của **Richard Sutton**.