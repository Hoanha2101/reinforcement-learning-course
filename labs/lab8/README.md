# Actor-Critic Algorithm for GridWorld

ÄÃ¢y lÃ  má»™t triá»ƒn khai thuáº­t toÃ¡n **Actor-Critic** Ä‘á»ƒ há»c chÃ­nh sÃ¡ch tá»‘i Æ°u trong mÃ´i trÆ°á»ng **GridWorld**.

## ğŸ›  CÃ i Ä‘áº·t

TrÆ°á»›c khi cháº¡y mÃ£, báº¡n cáº§n cÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t:

```bash
pip install numpy
```

## ğŸ“œ MÃ´ táº£

BÃ i toÃ¡n **GridWorld** mÃ´ phá»ng má»™t mÃ´i trÆ°á»ng dáº¡ng lÆ°á»›i 3x3, trong Ä‘Ã³ agent di chuyá»ƒn theo cÃ¡c hÆ°á»›ng **LÃªn, Xuá»‘ng, TrÃ¡i, Pháº£i** vÃ  nháº­n pháº§n thÆ°á»Ÿng khi Ä‘áº¿n Ã´ Ä‘Ã­ch.

### ğŸ¯ MÃ´i trÆ°á»ng
- LÆ°á»›i cÃ³ kÃ­ch thÆ°á»›c **3x3**.
- Sá»‘ hÃ nh Ä‘á»™ng cÃ³ thá»ƒ thá»±c hiá»‡n: **4 hÃ nh Ä‘á»™ng (LÃªn, Xuá»‘ng, TrÃ¡i, Pháº£i)**.
- Pháº§n thÆ°á»Ÿng:
  - Ã” (2,2) cÃ³ pháº§n thÆ°á»Ÿng **+1**.
  - CÃ¡c Ã´ cÃ²n láº¡i cÃ³ pháº§n thÆ°á»Ÿng **0**.
- Tráº¡ng thÃ¡i báº¯t Ä‘áº§u: **(0,0)**.
- Tráº¡ng thÃ¡i káº¿t thÃºc: **(2,2)**.

### ğŸ— CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng

1. **Táº¡o mÃ´i trÆ°á»ng GridWorld**:
   - XÃ¡c Ä‘á»‹nh kÃ­ch thÆ°á»›c lÆ°á»›i vÃ  pháº§n thÆ°á»Ÿng táº¡i tá»«ng Ã´.
   - Äá»‹nh nghÄ©a quy táº¯c di chuyá»ƒn (step function).

2. **Thuáº­t toÃ¡n Actor-Critic**:
   - **Critic**: Há»c giÃ¡ trá»‹ tráº¡ng thÃ¡i \( V(s) \) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hÃ nh Ä‘á»™ng.
   - **Actor**: Äiá»u chá»‰nh chÃ­nh sÃ¡ch xÃ¡c suáº¥t hÃ nh Ä‘á»™ng \( \pi(a | s) \).
   - **Cáº­p nháº­t Actor vÃ  Critic theo sai sá»‘ TD**:
     
     $$ \delta = r + \gamma V(s') - V(s) $$
     
     - **Critic Update**:
       
       $$ V(s) \leftarrow V(s) + \alpha_{critic} \cdot \delta $$
       
     - **Actor Update**:
       
       $$ \pi(a | s) \leftarrow \pi(a | s) + \alpha_{actor} \cdot \delta $$

3. **Huáº¥n luyá»‡n Actor-Critic**:
   - Khá»Ÿi táº¡o giÃ¡ trá»‹ tráº¡ng thÃ¡i vÃ  chÃ­nh sÃ¡ch hÃ nh Ä‘á»™ng.
   - Láº·p láº¡i trong `num_episodes` láº§n.
   - Chá»n hÃ nh Ä‘á»™ng theo **Softmax Policy**.
   - Cáº­p nháº­t Actor vÃ  Critic dá»±a trÃªn sai sá»‘ TD.
   - Káº¿t thÃºc khi Ä‘áº¿n tráº¡ng thÃ¡i má»¥c tiÃªu **(2,2)**.

## ğŸ”¢ Tham sá»‘ chÃ­nh

| Tham sá»‘ | GiÃ¡ trá»‹ |
|---------|--------|
| KÃ­ch thÆ°á»›c lÆ°á»›i | 3x3 |
| Sá»‘ hÃ nh Ä‘á»™ng | 4 |
| Sá»‘ táº­p huáº¥n luyá»‡n | 1000 |
| Learning rate (alpha_actor) | 0.1 |
| Learning rate (alpha_critic) | 0.1 |
| Discount factor (gamma) | 0.9 |

## â–¶ï¸ Cháº¡y chÆ°Æ¡ng trÃ¬nh

Äá»ƒ cháº¡y mÃ£, sá»­ dá»¥ng lá»‡nh sau trong terminal:

```bash
python actor_critic_gridworld.py
```

ChÆ°Æ¡ng trÃ¬nh sáº½ hiá»ƒn thá»‹ tá»•ng pháº§n thÆ°á»Ÿng thu Ä‘Æ°á»£c báº±ng chÃ­nh sÃ¡ch há»c Ä‘Æ°á»£c:

```
Total reward obtained by learned policy: X
```

(X lÃ  tá»•ng pháº§n thÆ°á»Ÿng thu Ä‘Æ°á»£c sau khi Ã¡p dá»¥ng chÃ­nh sÃ¡ch há»c Ä‘Æ°á»£c)

## ğŸ“ˆ Káº¿t quáº£ mong Ä‘á»£i
- **Critic** sáº½ há»c Ä‘Æ°á»£c giÃ¡ trá»‹ tráº¡ng thÃ¡i tá»‘i Æ°u.
- **Actor** sáº½ há»c Ä‘Æ°á»£c chÃ­nh sÃ¡ch xÃ¡c suáº¥t hÃ nh Ä‘á»™ng.
- **ChÃ­nh sÃ¡ch há»c Ä‘Æ°á»£c** sáº½ dáº«n agent Ä‘áº¿n tráº¡ng thÃ¡i má»¥c tiÃªu vá»›i pháº§n thÆ°á»Ÿng tá»‘i Æ°u.

## ğŸ“Œ Ghi chÃº
- Actor-Critic káº¿t há»£p giá»¯a **Value-Based (Critic)** vÃ  **Policy-Based (Actor)** Ä‘á»ƒ cáº£i thiá»‡n chÃ­nh sÃ¡ch.
- Báº¡n cÃ³ thá»ƒ thay Ä‘á»•i **alpha_actor** vÃ  **alpha_critic** Ä‘á»ƒ kiá»ƒm tra áº£nh hÆ°á»Ÿng cá»§a tá»‘c Ä‘á»™ há»c.
- Softmax Policy Ä‘áº£m báº£o viá»‡c chá»n hÃ nh Ä‘á»™ng **cÃ³ tÃ­nh xÃ¡c suáº¥t**, giÃºp trÃ¡nh tá»‘i Æ°u cá»¥c bá»™.

## ğŸ“œ Báº£n quyá»n
MÃ£ nguá»“n Ä‘Æ°á»£c cung cáº¥p miá»…n phÃ­ vÃ  cÃ³ thá»ƒ sá»­ dá»¥ng cho má»¥c Ä‘Ã­ch há»c táº­p hoáº·c nghiÃªn cá»©u.

