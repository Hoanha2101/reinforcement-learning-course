{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlsyV0_QqNmU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value function:\n",
            "[[-5.45739943 -4.09230234 -1.69585506]\n",
            " [-4.88224585 -3.66516276  3.02507848]\n",
            " [-3.93007019 -0.48226552  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Lab 2.2 implement Temporal Difference- HoaDNt@fe.edu.vn\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        \n",
        "        # [0, 0, 0], \n",
        "        # [0, 0, 0], \n",
        "        # [0, 0, 1]\n",
        "        \n",
        "        self.grid_size = (3, 3)\n",
        "        self.start_state = (0, 0)\n",
        "        self.goal_state = (2, 2)\n",
        "        self.num_actions = 4  # Up, Down, Left, Right\n",
        "\n",
        "    def step(self, state, action):\n",
        "        row, col = state\n",
        "        if action == 0:  # Up\n",
        "            row = max(0, row - 1)\n",
        "        elif action == 1:  # Down\n",
        "            row = min(self.grid_size[0] - 1, row + 1)\n",
        "        elif action == 2:  # Left\n",
        "            col = max(0, col - 1)\n",
        "        elif action == 3:  # Right\n",
        "            col = min(self.grid_size[1] - 1, col + 1)\n",
        "\n",
        "        next_state = (row, col)\n",
        "        reward = -1\n",
        "        if next_state == self.goal_state:\n",
        "            reward = 10\n",
        "        return next_state, reward\n",
        "\n",
        "def td_learning(grid_world, num_episodes, alpha, gamma):\n",
        "    V = np.zeros(grid_world.grid_size)  # Value function initialization\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = grid_world.start_state\n",
        "        while state != grid_world.goal_state:\n",
        "            action = np.random.randint(grid_world.num_actions)  # Random action\n",
        "            next_state, reward = grid_world.step(state, action)\n",
        "            V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n",
        "            state = next_state\n",
        "    return V\n",
        "\n",
        "# Create a grid world environment\n",
        "grid_world = GridWorld()\n",
        "\n",
        "# Perform Temporal Difference learning\n",
        "num_episodes = 1000\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "values = td_learning(grid_world, num_episodes, alpha, gamma)\n",
        "\n",
        "print(\"Value function:\")\n",
        "print(values)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
